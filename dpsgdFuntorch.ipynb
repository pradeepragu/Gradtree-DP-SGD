{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYyYKASeSvvs8bVKW/iRRW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pradeepragu/Gradtree-DP-SGD/blob/main/dpsgdFuntorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Ql--xU0RpcRk",
        "outputId": "af1e4c09-e34b-45ae-922a-128a4b394594"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1. select a batch of samples size L by selecting eachh sample to be in thhe lot independentity with probability L/n\\n2. for each samplee (x,y) in the batch, compute the gradient\\n3. Clip the the gradient to havee L2 norm at most C\\n4. Average the clipped gradients \\n5. Add Gaussian noise\\n6. Take a step in the negative direction of the resulting vector\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\"\"\"\n",
        "1. select a batch of samples size L by selecting eachh sample to be in thhe lot independentity with probability L/n\n",
        "2. for each samplee (x,y) in the batch, compute the gradient\n",
        "3. Clip the the gradient to havee L2 norm at most C\n",
        "4. Average the clipped gradients\n",
        "5. Add Gaussian noise\n",
        "6. Take a step in the negative direction of the resulting vector\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opacus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXCVsagzTUQ9",
        "outputId": "cd93c35a-fcca-409f-d132-dc45bf701043"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opacus\n",
            "  Downloading opacus-1.5.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.26.4)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (2.4.1+cu121)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.13.1)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->opacus) (1.3.0)\n",
            "Downloading opacus-1.5.2-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.9/239.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opacus\n",
            "Successfully installed opacus-1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Preprocessing steps for image data (e.g., CIFAR-10)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert the image to PyTorch tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image between [-1, 1]\n",
        "])\n",
        "\n",
        "# Download the CIFAR-10 dataset and split into training and test sets\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoader for training and testing\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnVHKAPC5Su_",
        "outputId": "1bf06d4d-f86e-414f-dc25-5e47e576067e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 57546698.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 256)  # CIFAR-10 images are 32x32; after pooling twice, size reduces to 8x8\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)  # 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Conv1 + ReLU + Pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # Conv2 + ReLU + Pooling\n",
        "        x = x.view(-1, 64 * 8 * 8)  # Flatten\n",
        "        x = F.relu(self.fc1(x))  # Fully connected layer\n",
        "        x = F.relu(self.fc2(x))  # Fully connected layer\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "yGMPgHs-5X-z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "\n",
        "# Loss function\n",
        "def loss_fn(predictions, targets):\n",
        "    return F.cross_entropy(predictions, targets)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model, optimizer, and other hyperparameters\n",
        "model = CNN().to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "# DP-SGD parameters\n",
        "\n",
        "max_grad_norm = 1.0\n",
        "target_epsilon = 1.0  # Privacy budget target (fixed at 1,3,6)\n",
        "delta = 1e-5          # Delta value\n",
        "sample_rate = batch_size / len(train_loader.dataset)\n",
        "num_epochs = 50\n",
        "MAX_PHYSICAL_BATCH_SIZE = 256"
      ],
      "metadata": {
        "id": "KQ1OY6Km64oe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the GaussianAccountant class from your accountant file\n",
        "import warnings\n",
        "from opacus.accountants import IAccountant\n",
        "from opacus.accountants.analysis import gdp as privacy_analysis\n",
        "from opacus.accountants import create_accountant\n",
        "from typing import Optional\n",
        "class GaussianAccountant(IAccountant):\n",
        "    def __init__(self):\n",
        "        warnings.warn(\n",
        "            \"GDP accounting is experimental and can underestimate privacy expenditure.\"\n",
        "            \"Proceed with caution. More details: https://arxiv.org/pdf/2106.02848.pdf\"\n",
        "        )\n",
        "        super().__init__()\n",
        "\n",
        "    def step(self, *, noise_multiplier: float, sample_rate: float):\n",
        "        if len(self.history) >= 1:\n",
        "            last_noise_multiplier, last_sample_rate, num_steps = self.history.pop()\n",
        "            if (\n",
        "                last_noise_multiplier != noise_multiplier\n",
        "                or last_sample_rate != sample_rate\n",
        "            ):\n",
        "                raise ValueError(\n",
        "                    \"Noise multiplier and sample rate have to stay constant in GaussianAccountant.\"\n",
        "                )\n",
        "            else:\n",
        "                self.history = [\n",
        "                    (last_noise_multiplier, last_sample_rate, num_steps + 1)\n",
        "                ]\n",
        "\n",
        "        else:\n",
        "            self.history = [(noise_multiplier, sample_rate, 1)]\n",
        "\n",
        "    def get_epsilon(self, delta: float, poisson: bool = True) -> float:\n",
        "        \"\"\"\n",
        "        Return privacy budget (epsilon) expended so far.\n",
        "\n",
        "        Args:\n",
        "            delta: target delta\n",
        "            poisson: ``True`` is input batches was sampled via Poisson sampling,\n",
        "                ``False`` otherwise\n",
        "        \"\"\"\n",
        "\n",
        "        compute_eps = (\n",
        "            privacy_analysis.compute_eps_poisson\n",
        "            if poisson\n",
        "            else privacy_analysis.compute_eps_uniform\n",
        "        )\n",
        "        noise_multiplier, sample_rate, num_steps = self.history[-1]\n",
        "        return compute_eps(\n",
        "            steps=num_steps,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            sample_rate=sample_rate,\n",
        "            delta=delta,\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.history)\n",
        "\n",
        "    @classmethod\n",
        "    def mechanism(cls) -> str:\n",
        "        return \"gdp\"\n",
        "\n",
        "\n",
        "# Initialize the GaussianAccountant\n",
        "accountant = GaussianAccountant()\n",
        "\n",
        "MAX_SIGMA = 1e6\n",
        "\n",
        "# Get noise multiplier function\n",
        "def get_noise_multiplier(\n",
        "    *,\n",
        "    target_epsilon: float,\n",
        "    target_delta: float,\n",
        "    sample_rate: float,\n",
        "    epochs: Optional[int] = None,\n",
        "    steps: Optional[int] = None,\n",
        "    accountant: str = \"gdp\",\n",
        "    epsilon_tolerance: float = 0.01,\n",
        "    **kwargs,\n",
        ") -> float:\n",
        "    if (steps is None) == (epochs is None):\n",
        "        raise ValueError(\"get_noise_multiplier takes as input EITHER a number of steps or a number of epochs\")\n",
        "    if steps is None:\n",
        "        steps = int(epochs / sample_rate)\n",
        "\n",
        "    eps_high = float(\"inf\")\n",
        "    accountant = create_accountant(mechanism=accountant)\n",
        "\n",
        "    sigma_low, sigma_high = 0, 10\n",
        "    while eps_high > target_epsilon:\n",
        "        sigma_high = 2 * sigma_high\n",
        "        accountant.history = [(sigma_high, sample_rate, steps)]\n",
        "        eps_high = accountant.get_epsilon(delta=target_delta, **kwargs)\n",
        "        if sigma_high > MAX_SIGMA:\n",
        "            raise ValueError(\"The privacy budget is too low.\")\n",
        "\n",
        "    while target_epsilon - eps_high > epsilon_tolerance:\n",
        "        sigma = (sigma_low + sigma_high) / 2\n",
        "        accountant.history = [(sigma, sample_rate, steps)]\n",
        "        eps = accountant.get_epsilon(delta=target_delta, **kwargs)\n",
        "\n",
        "        if eps < target_epsilon:\n",
        "            sigma_high = sigma\n",
        "            eps_high = eps\n",
        "        else:\n",
        "            sigma_low = sigma\n",
        "\n",
        "    return sigma_high\n",
        "# Calculate the noise multiplier before training using the total number of steps\n",
        "noise_multiplier = get_noise_multiplier(\n",
        "    target_epsilon=target_epsilon,\n",
        "    target_delta=delta,\n",
        "    sample_rate=sample_rate,\n",
        "    epochs=num_epochs,\n",
        ")\n",
        "print(f\"Calculated noise multiplier for fixed privacy budget: {noise_multiplier}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p_prhz76q0Z",
        "outputId": "0a0efd20-9247-4afb-fae4-ad3638d6cf08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated noise multiplier for fixed privacy budget: 1.1572265625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-9d430db62b61>:9: UserWarning: GDP accounting is experimental and can underestimate privacy expenditure.Proceed with caution. More details: https://arxiv.org/pdf/2106.02848.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/opacus/accountants/gdp.py:23: UserWarning: GDP accounting is experimental and can underestimate privacy expenditure.Proceed with caution. More details: https://arxiv.org/pdf/2106.02848.pdf\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functorch import make_functional_with_buffers, vmap, grad\n",
        "#This will separate state (the parameters) from the model and turn the model into a pure function\n",
        "fmodel, params, buffers = make_functional_with_buffers(model)\n",
        "#the model has become the stateless FunctionalModuleWithBuffers\n",
        "fmodel\n",
        "#the model parameters now exist independently of the model, stored as a tuple\n",
        "for x in params:\n",
        "  print(f\"{x.shape}\")\n",
        "\n",
        "print(f\"\\n{type(params)}\")\n",
        "#function to compute the loss of the model given a single input rather than a batch of inputs\n",
        "def compute_loss_stateless_model (params, buffers, sample, target):\n",
        "    batch = sample.unsqueeze(0)\n",
        "    targets = target.unsqueeze(0)\n",
        "\n",
        "    predictions = fmodel(params, buffers, batch)\n",
        "    loss = loss_fn(predictions, targets)\n",
        "    return loss\n",
        "#let’s use functorch’s grad to create a new function that computes the gradient with respect to the first argument of compute_loss (i.e. the params).\n",
        "ft_compute_grad = grad(compute_loss_stateless_model)\n",
        "#ft_compute_grad was used to find the gradient for single sample. now using vmap we extend that to entire batch\n",
        "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWLnOT9fhb06",
        "outputId": "4f3e8748-6d78-4393-b911-ad404e0b4b10"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 3, 3])\n",
            "torch.Size([32])\n",
            "torch.Size([64, 32, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([256, 4096])\n",
            "torch.Size([256])\n",
            "torch.Size([128, 256])\n",
            "torch.Size([128])\n",
            "torch.Size([10, 128])\n",
            "torch.Size([10])\n",
            "\n",
            "<class 'tuple'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b53a6d5af22e>:3: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.make_functional_with_buffers` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.functional_call` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
            "  fmodel, params, buffers = make_functional_with_buffers(model)\n",
            "<ipython-input-6-b53a6d5af22e>:20: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.grad` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.grad` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
            "  ft_compute_grad = grad(compute_loss_stateless_model)\n",
            "<ipython-input-6-b53a6d5af22e>:22: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
            "  ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the clip_gradients function to handle single examples\n",
        "def clip_gradients_and_add_noise(params, max_grad_norm, per_sample_grads):\n",
        "    \"\"\"Clip per-sample gradients to a maximum L2 norm.\"\"\"\n",
        "    # Compute the L2 norm of each per-sample gradient\n",
        "    total_grad_norms = torch.stack([\n",
        "        torch.norm(torch.stack([torch.norm(g) for g in sample_grads]))\n",
        "        for sample_grads in zip(*per_sample_grads)\n",
        "    ])\n",
        "\n",
        "    # Compute the clipping factor for each sample\n",
        "    clip_coef = max_grad_norm / (total_grad_norms + 1e-6)\n",
        "    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
        "\n",
        "    # Clip the gradients\n",
        "    clipped_grads = [\n",
        "        [g * c for g in sample_grads]\n",
        "        for sample_grads, c in zip(zip(*per_sample_grads), clip_coef_clamped)\n",
        "    ]\n",
        "\n",
        "    # Corrected gradient aggregation step\n",
        "    aggregated_grads = [torch.sum(torch.stack(grad), dim=0) for grad in zip(*clipped_grads)]\n",
        "\n",
        "    # Add noise to the aggregated gradients\n",
        "    noised_grads = [\n",
        "        g + noise_multiplier * max_grad_norm * torch.randn_like(g) / batch_size\n",
        "        for g in aggregated_grads\n",
        "    ]\n",
        "\n",
        "    return noised_grads\n"
      ],
      "metadata": {
        "id": "CSuh8X0aPPP9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, labels):\n",
        "    return (preds == labels).mean()"
      ],
      "metadata": {
        "id": "dp5wW8fNC1oJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the train_step function\n",
        "\n",
        "def train_step(model, optimizer, data, targets):\n",
        "    per_sample_grads = ft_compute_sample_grad(params, buffers, data, targets)\n",
        "    noised_grads = clip_gradients_and_add_noise(params, max_grad_norm, per_sample_grads)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    for param, noised_grad in zip(model.parameters(), noised_grads):\n",
        "        param.grad = noised_grad\n",
        "    optimizer.step()\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    return test_loss, accuracy\n"
      ],
      "metadata": {
        "id": "eWT9iLQWheUi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "steps = 0\n",
        "losses = []\n",
        "epsilons = []\n",
        "train_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):  # Train for full epochs\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Perform a training step\n",
        "        train_step(model, optimizer, data, target)\n",
        "        steps += 1\n",
        "\n",
        "        # Update accountant with current noise multiplier and sample rate\n",
        "        accountant.step(noise_multiplier=noise_multiplier, sample_rate=sample_rate)\n",
        "\n",
        "        # Compute loss for the current batch\n",
        "        outputs = model(data)\n",
        "        loss = loss_fn(outputs, target)\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Compute the number of correct predictions in the training batch\n",
        "        pred = outputs.argmax(dim=1, keepdim=True)\n",
        "        correct_train += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total_train += target.size(0)\n",
        "\n",
        "    # Compute average loss over the training data\n",
        "    losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = 100. * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss, test_accuracy = test(model, device, test_loader)\n",
        "\n",
        "    # Get the current epsilon from GaussianAccountant\n",
        "    epsilon = accountant.get_epsilon(delta=delta, poisson=True)  # Use Poisson sampling\n",
        "    epsilons.append(epsilon)\n",
        "\n",
        "    # Print results for the current epoch\n",
        "    print(f'Epoch {epoch+1}: Train Loss: {epoch_loss / len(train_loader):.4f}, '\n",
        "          f'Train Accuracy: {train_accuracy:.2f}%, '\n",
        "          f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, '\n",
        "          f'Epsilon: {epsilon:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "AOylWFjN7R01",
        "outputId": "2e432404-0e1a-4953-855e-5ae7de8e88b9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 64.1244, Train Accuracy: 10.67%, Test Loss: 303.5601, Test Accuracy: 10.00%, Epsilon: 0.1176\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b2f9db5c2625>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Perform a training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-5b20e2a922a6>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, data, targets)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mper_sample_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_compute_sample_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnoised_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_gradients_and_add_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_sample_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-bb855962f13a>\u001b[0m in \u001b[0;36mclip_gradients_and_add_noise\u001b[0;34m(params, max_grad_norm, per_sample_grads)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Corrected gradient aggregation step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0maggregated_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mclipped_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Add noise to the aggregated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-bb855962f13a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Corrected gradient aggregation step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0maggregated_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mclipped_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Add noise to the aggregated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}