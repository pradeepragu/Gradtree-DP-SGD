{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWivMdilEV5O",
        "outputId": "aa130d24-3876-4a09-f49a-a1e2faac13fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-d15a1a4069dd>:103: UserWarning: GDP accounting is experimental and can underestimate privacy expenditure.Proceed with caution. More details: https://arxiv.org/pdf/2106.02848.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/opacus/accountants/gdp.py:23: UserWarning: GDP accounting is experimental and can underestimate privacy expenditure.Proceed with caution. More details: https://arxiv.org/pdf/2106.02848.pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculated noise multiplier for fixed privacy budget: 5.7421875\n",
            "Epoch 1: Train Loss: 1.3915, Train Accuracy: 28.22%, Test Loss: 1.2893, Test Accuracy: 61.56%, Epsilon: 0.1190\n",
            "Epoch 2: Train Loss: 1.1802, Train Accuracy: 70.04%, Test Loss: 1.1063, Test Accuracy: 67.92%, Epsilon: 0.1737\n",
            "Epoch 3: Train Loss: 1.0179, Train Accuracy: 70.55%, Test Loss: 0.9841, Test Accuracy: 67.92%, Epsilon: 0.2167\n",
            "Epoch 4: Train Loss: 0.9214, Train Accuracy: 70.55%, Test Loss: 0.9160, Test Accuracy: 67.92%, Epsilon: 0.2535\n",
            "Epoch 5: Train Loss: 0.8604, Train Accuracy: 70.55%, Test Loss: 0.8747, Test Accuracy: 67.92%, Epsilon: 0.2862\n",
            "Epoch 6: Train Loss: 0.8289, Train Accuracy: 70.55%, Test Loss: 0.8482, Test Accuracy: 67.92%, Epsilon: 0.3161\n",
            "Epoch 7: Train Loss: 0.8031, Train Accuracy: 70.55%, Test Loss: 0.8280, Test Accuracy: 67.92%, Epsilon: 0.3438\n",
            "Epoch 8: Train Loss: 0.7873, Train Accuracy: 70.55%, Test Loss: 0.8131, Test Accuracy: 67.92%, Epsilon: 0.3697\n",
            "Epoch 9: Train Loss: 0.7752, Train Accuracy: 70.55%, Test Loss: 0.7981, Test Accuracy: 67.92%, Epsilon: 0.3942\n",
            "Epoch 10: Train Loss: 0.7612, Train Accuracy: 70.55%, Test Loss: 0.7853, Test Accuracy: 67.92%, Epsilon: 0.4175\n",
            "Epoch 11: Train Loss: 0.7464, Train Accuracy: 70.55%, Test Loss: 0.7742, Test Accuracy: 67.92%, Epsilon: 0.4398\n",
            "Epoch 12: Train Loss: 0.7328, Train Accuracy: 70.55%, Test Loss: 0.7662, Test Accuracy: 68.50%, Epsilon: 0.4611\n",
            "Epoch 13: Train Loss: 0.7322, Train Accuracy: 70.77%, Test Loss: 0.7587, Test Accuracy: 68.79%, Epsilon: 0.4817\n",
            "Epoch 14: Train Loss: 0.7229, Train Accuracy: 71.35%, Test Loss: 0.7505, Test Accuracy: 68.79%, Epsilon: 0.5016\n",
            "Epoch 15: Train Loss: 0.7160, Train Accuracy: 71.27%, Test Loss: 0.7444, Test Accuracy: 68.21%, Epsilon: 0.5208\n",
            "Epoch 16: Train Loss: 0.7077, Train Accuracy: 71.27%, Test Loss: 0.7398, Test Accuracy: 67.92%, Epsilon: 0.5394\n",
            "Epoch 17: Train Loss: 0.7073, Train Accuracy: 70.84%, Test Loss: 0.7337, Test Accuracy: 67.92%, Epsilon: 0.5576\n",
            "Epoch 18: Train Loss: 0.7013, Train Accuracy: 70.84%, Test Loss: 0.7296, Test Accuracy: 67.63%, Epsilon: 0.5752\n",
            "Epoch 19: Train Loss: 0.6899, Train Accuracy: 71.06%, Test Loss: 0.7254, Test Accuracy: 67.05%, Epsilon: 0.5925\n",
            "Epoch 20: Train Loss: 0.6927, Train Accuracy: 71.20%, Test Loss: 0.7224, Test Accuracy: 67.05%, Epsilon: 0.6093\n",
            "Epoch 21: Train Loss: 0.6872, Train Accuracy: 71.13%, Test Loss: 0.7197, Test Accuracy: 67.05%, Epsilon: 0.6257\n",
            "Epoch 22: Train Loss: 0.6828, Train Accuracy: 71.42%, Test Loss: 0.7166, Test Accuracy: 66.76%, Epsilon: 0.6418\n",
            "Epoch 23: Train Loss: 0.6818, Train Accuracy: 71.27%, Test Loss: 0.7144, Test Accuracy: 67.63%, Epsilon: 0.6576\n",
            "Epoch 24: Train Loss: 0.6784, Train Accuracy: 70.98%, Test Loss: 0.7143, Test Accuracy: 67.63%, Epsilon: 0.6731\n",
            "Epoch 25: Train Loss: 0.6766, Train Accuracy: 71.13%, Test Loss: 0.7118, Test Accuracy: 67.63%, Epsilon: 0.6882\n",
            "Epoch 26: Train Loss: 0.6747, Train Accuracy: 71.35%, Test Loss: 0.7087, Test Accuracy: 67.92%, Epsilon: 0.7032\n",
            "Epoch 27: Train Loss: 0.6719, Train Accuracy: 71.42%, Test Loss: 0.7076, Test Accuracy: 67.92%, Epsilon: 0.7178\n",
            "Epoch 28: Train Loss: 0.6704, Train Accuracy: 71.27%, Test Loss: 0.7068, Test Accuracy: 67.63%, Epsilon: 0.7322\n",
            "Epoch 29: Train Loss: 0.6695, Train Accuracy: 71.49%, Test Loss: 0.7040, Test Accuracy: 67.92%, Epsilon: 0.7464\n",
            "Epoch 30: Train Loss: 0.6683, Train Accuracy: 71.27%, Test Loss: 0.7013, Test Accuracy: 67.92%, Epsilon: 0.7604\n",
            "Epoch 31: Train Loss: 0.6635, Train Accuracy: 71.71%, Test Loss: 0.6986, Test Accuracy: 67.63%, Epsilon: 0.7742\n",
            "Epoch 32: Train Loss: 0.6652, Train Accuracy: 71.78%, Test Loss: 0.6949, Test Accuracy: 67.92%, Epsilon: 0.7877\n",
            "Epoch 33: Train Loss: 0.6567, Train Accuracy: 71.78%, Test Loss: 0.6931, Test Accuracy: 67.92%, Epsilon: 0.8011\n",
            "Epoch 34: Train Loss: 0.6606, Train Accuracy: 71.64%, Test Loss: 0.6924, Test Accuracy: 68.21%, Epsilon: 0.8143\n",
            "Epoch 35: Train Loss: 0.6565, Train Accuracy: 71.78%, Test Loss: 0.6917, Test Accuracy: 68.21%, Epsilon: 0.8273\n",
            "Epoch 36: Train Loss: 0.6579, Train Accuracy: 71.56%, Test Loss: 0.6887, Test Accuracy: 67.63%, Epsilon: 0.8402\n",
            "Epoch 37: Train Loss: 0.6541, Train Accuracy: 71.85%, Test Loss: 0.6870, Test Accuracy: 67.63%, Epsilon: 0.8529\n",
            "Epoch 38: Train Loss: 0.6488, Train Accuracy: 72.00%, Test Loss: 0.6848, Test Accuracy: 68.21%, Epsilon: 0.8654\n",
            "Epoch 39: Train Loss: 0.6473, Train Accuracy: 71.85%, Test Loss: 0.6815, Test Accuracy: 67.92%, Epsilon: 0.8778\n",
            "Epoch 40: Train Loss: 0.6495, Train Accuracy: 72.14%, Test Loss: 0.6778, Test Accuracy: 67.92%, Epsilon: 0.8901\n",
            "Epoch 41: Train Loss: 0.6456, Train Accuracy: 72.29%, Test Loss: 0.6763, Test Accuracy: 68.21%, Epsilon: 0.9022\n",
            "Epoch 42: Train Loss: 0.6388, Train Accuracy: 72.14%, Test Loss: 0.6743, Test Accuracy: 67.92%, Epsilon: 0.9142\n",
            "Epoch 43: Train Loss: 0.6359, Train Accuracy: 72.29%, Test Loss: 0.6714, Test Accuracy: 68.50%, Epsilon: 0.9261\n",
            "Epoch 44: Train Loss: 0.6357, Train Accuracy: 72.21%, Test Loss: 0.6700, Test Accuracy: 68.21%, Epsilon: 0.9379\n",
            "Epoch 45: Train Loss: 0.6348, Train Accuracy: 72.00%, Test Loss: 0.6692, Test Accuracy: 68.21%, Epsilon: 0.9495\n",
            "Epoch 46: Train Loss: 0.6352, Train Accuracy: 72.72%, Test Loss: 0.6661, Test Accuracy: 68.50%, Epsilon: 0.9610\n",
            "Epoch 47: Train Loss: 0.6320, Train Accuracy: 72.58%, Test Loss: 0.6635, Test Accuracy: 68.50%, Epsilon: 0.9724\n",
            "Epoch 48: Train Loss: 0.6288, Train Accuracy: 72.43%, Test Loss: 0.6616, Test Accuracy: 68.79%, Epsilon: 0.9837\n",
            "Epoch 49: Train Loss: 0.6250, Train Accuracy: 72.43%, Test Loss: 0.6585, Test Accuracy: 68.50%, Epsilon: 0.9949\n",
            "Epoch 50: Train Loss: 0.6256, Train Accuracy: 72.65%, Test Loss: 0.6563, Test Accuracy: 68.21%, Epsilon: 1.0060\n"
          ]
        }
      ],
      "source": [
        "#fixed privacy budget for all epochs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from opacus.accountants import create_accountant\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "car_data = pd.read_csv('/content/car.data', header=None)\n",
        "\n",
        "# Preprocessing steps:\n",
        "features = car_data.iloc[:, :-1]\n",
        "targets = car_data.iloc[:, -1]\n",
        "\n",
        "# Convert categorical features and targets into numerical values\n",
        "encoder = LabelEncoder()\n",
        "features = features.apply(encoder.fit_transform)\n",
        "targets = encoder.fit_transform(targets)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create a custom dataset class\n",
        "class CarDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create DataLoader for training and testing\n",
        "batch_size = 64\n",
        "train_dataset = CarDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = CarDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the NN model for tabular data\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Loss function\n",
        "def loss_fn(predictions, targets):\n",
        "    return F.cross_entropy(predictions, targets)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model, optimizer, and other hyperparameters\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = len(set(y_train))\n",
        "model = NN(input_dim, output_dim).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# DP-SGD parameters\n",
        "max_grad_norm = 1.0\n",
        "target_epsilon = 1.0  # Privacy budget target (fixed at 3)\n",
        "delta = 1e-5          # Delta value\n",
        "sample_rate = batch_size / len(train_dataset)\n",
        "num_epochs = 50       # Train for full epochs\n",
        "\n",
        "# Import the GaussianAccountant class from your accountant file\n",
        "import warnings\n",
        "from opacus.accountants import IAccountant\n",
        "from opacus.accountants.analysis import gdp as privacy_analysis\n",
        "\n",
        "class GaussianAccountant(IAccountant):\n",
        "    def __init__(self):\n",
        "        warnings.warn(\n",
        "            \"GDP accounting is experimental and can underestimate privacy expenditure.\"\n",
        "            \"Proceed with caution. More details: https://arxiv.org/pdf/2106.02848.pdf\"\n",
        "        )\n",
        "        super().__init__()\n",
        "\n",
        "    def step(self, *, noise_multiplier: float, sample_rate: float):\n",
        "        if len(self.history) >= 1:\n",
        "            last_noise_multiplier, last_sample_rate, num_steps = self.history.pop()\n",
        "            if (\n",
        "                last_noise_multiplier != noise_multiplier\n",
        "                or last_sample_rate != sample_rate\n",
        "            ):\n",
        "                raise ValueError(\n",
        "                    \"Noise multiplier and sample rate have to stay constant in GaussianAccountant.\"\n",
        "                )\n",
        "            else:\n",
        "                self.history = [\n",
        "                    (last_noise_multiplier, last_sample_rate, num_steps + 1)\n",
        "                ]\n",
        "\n",
        "        else:\n",
        "            self.history = [(noise_multiplier, sample_rate, 1)]\n",
        "\n",
        "    def get_epsilon(self, delta: float, poisson: bool = True) -> float:\n",
        "        \"\"\"\n",
        "        Return privacy budget (epsilon) expended so far.\n",
        "\n",
        "        Args:\n",
        "            delta: target delta\n",
        "            poisson: ``True`` is input batches was sampled via Poisson sampling,\n",
        "                ``False`` otherwise\n",
        "        \"\"\"\n",
        "\n",
        "        compute_eps = (\n",
        "            privacy_analysis.compute_eps_poisson\n",
        "            if poisson\n",
        "            else privacy_analysis.compute_eps_uniform\n",
        "        )\n",
        "        noise_multiplier, sample_rate, num_steps = self.history[-1]\n",
        "        return compute_eps(\n",
        "            steps=num_steps,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            sample_rate=sample_rate,\n",
        "            delta=delta,\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.history)\n",
        "\n",
        "    @classmethod\n",
        "    def mechanism(cls) -> str:\n",
        "        return \"gdp\"\n",
        "\n",
        "\n",
        "# Initialize the GaussianAccountant\n",
        "accountant = GaussianAccountant()\n",
        "\n",
        "MAX_SIGMA = 1e6\n",
        "\n",
        "# Get noise multiplier function\n",
        "def get_noise_multiplier(\n",
        "    *,\n",
        "    target_epsilon: float,\n",
        "    target_delta: float,\n",
        "    sample_rate: float,\n",
        "    epochs: Optional[int] = None,\n",
        "    steps: Optional[int] = None,\n",
        "    accountant: str = \"gdp\",\n",
        "    epsilon_tolerance: float = 0.01,\n",
        "    **kwargs,\n",
        ") -> float:\n",
        "    if (steps is None) == (epochs is None):\n",
        "        raise ValueError(\"get_noise_multiplier takes as input EITHER a number of steps or a number of epochs\")\n",
        "    if steps is None:\n",
        "        steps = int(epochs / sample_rate)\n",
        "\n",
        "    eps_high = float(\"inf\")\n",
        "    accountant = create_accountant(mechanism=accountant)\n",
        "\n",
        "    sigma_low, sigma_high = 0, 10\n",
        "    while eps_high > target_epsilon:\n",
        "        sigma_high = 2 * sigma_high\n",
        "        accountant.history = [(sigma_high, sample_rate, steps)]\n",
        "        eps_high = accountant.get_epsilon(delta=target_delta, **kwargs)\n",
        "        if sigma_high > MAX_SIGMA:\n",
        "            raise ValueError(\"The privacy budget is too low.\")\n",
        "\n",
        "    while target_epsilon - eps_high > epsilon_tolerance:\n",
        "        sigma = (sigma_low + sigma_high) / 2\n",
        "        accountant.history = [(sigma, sample_rate, steps)]\n",
        "        eps = accountant.get_epsilon(delta=target_delta, **kwargs)\n",
        "\n",
        "        if eps < target_epsilon:\n",
        "            sigma_high = sigma\n",
        "            eps_high = eps\n",
        "        else:\n",
        "            sigma_low = sigma\n",
        "\n",
        "    return sigma_high\n",
        "\n",
        "# Calculate the noise multiplier before training using the total number of steps\n",
        "noise_multiplier = get_noise_multiplier(\n",
        "    target_epsilon=target_epsilon,\n",
        "    target_delta=delta,\n",
        "    sample_rate=sample_rate,\n",
        "    epochs=num_epochs,\n",
        ")\n",
        "\n",
        "print(f\"Calculated noise multiplier for fixed privacy budget: {noise_multiplier}\")\n",
        "\n",
        "#  functions for DP-SGD\n",
        "def compute_per_sample_gradients(model, loss_fn, data, targets):\n",
        "    model.zero_grad()\n",
        "    outputs = model(data)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    per_sample_grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "    return per_sample_grads\n",
        "\n",
        "def clip_gradients(gradients, max_norm):\n",
        "    total_norm = torch.norm(torch.stack([torch.norm(g.detach(), p=2) for g in gradients]))\n",
        "    clip_coef = max_norm / (total_norm + 1e-6)\n",
        "    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
        "    return [g.detach() * clip_coef_clamped for g in gradients]\n",
        "\n",
        "def add_noise(gradients, noise_multiplier, max_norm):\n",
        "    noised_gradients = []\n",
        "    for grad in gradients:\n",
        "        noise = torch.normal(0, noise_multiplier * max_norm / batch_size, grad.shape, device=grad.device)\n",
        "        noised_gradients.append(grad + noise)\n",
        "    return noised_gradients\n",
        "\n",
        "def train_step(model, optimizer, data, targets):\n",
        "    per_sample_grads = compute_per_sample_gradients(model, loss_fn, data, targets)\n",
        "    clipped_grads = clip_gradients(per_sample_grads, max_grad_norm)\n",
        "    noised_grads = add_noise(clipped_grads, noise_multiplier, max_grad_norm)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    for param, noised_grad in zip(model.parameters(), noised_grads):\n",
        "        param.grad = noised_grad\n",
        "    optimizer.step()\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    return test_loss, accuracy\n",
        "\n",
        "# Training loop\n",
        "steps = 0\n",
        "losses = []\n",
        "epsilons = []\n",
        "train_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):  # Train for full epochs\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Perform a training step\n",
        "        train_step(model, optimizer, data, target)\n",
        "        steps += 1\n",
        "\n",
        "        # Update accountant with current noise multiplier and sample rate\n",
        "        accountant.step(noise_multiplier=noise_multiplier, sample_rate=sample_rate)\n",
        "\n",
        "        # Compute loss for the current batch\n",
        "        outputs = model(data)\n",
        "        loss = loss_fn(outputs, target)\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Compute the number of correct predictions in the training batch\n",
        "        pred = outputs.argmax(dim=1, keepdim=True)\n",
        "        correct_train += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total_train += target.size(0)\n",
        "\n",
        "    # Compute average loss over the training data\n",
        "    losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = 100. * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss, test_accuracy = test(model, device, test_loader)\n",
        "\n",
        "    # Get the current epsilon from GaussianAccountant\n",
        "    epsilon = accountant.get_epsilon(delta=delta, poisson=True)  # Use Poisson sampling\n",
        "    epsilons.append(epsilon)\n",
        "\n",
        "    # Print results for the current epoch\n",
        "    print(f'Epoch {epoch+1}: Train Loss: {epoch_loss / len(train_loader):.4f}, '\n",
        "          f'Train Accuracy: {train_accuracy:.2f}%, '\n",
        "          f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, '\n",
        "          f'Epsilon: {epsilon:.4f}')\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
