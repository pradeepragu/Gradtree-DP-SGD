{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0XabOBVPtRw"
      },
      "outputs": [],
      "source": [
        "#manually adjust noise multipliier to get realistic privacy budget, check readme file for the noise multipler values\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "car_data = pd.read_csv('/content/car.data', header=None)\n",
        "\n",
        "# Assume the last column is the target, and the rest are features.\n",
        "features = car_data.iloc[:, :-1]\n",
        "targets = car_data.iloc[:, -1]\n",
        "\n",
        "# Convert categorical features and targets into numerical values\n",
        "encoder = LabelEncoder()\n",
        "features = features.apply(encoder.fit_transform)\n",
        "targets = encoder.fit_transform(targets)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create a custom dataset class\n",
        "class CarDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create DataLoader for training and testing\n",
        "batch_size = 64\n",
        "train_dataset = CarDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = CarDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the NN model for tabular data\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Loss function\n",
        "def loss_fn(predictions, targets):\n",
        "    return F.cross_entropy(predictions, targets)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model, optimizer, and other hyperparameters\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = len(set(y_train))\n",
        "model = SimpleMLP(input_dim, output_dim).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# DP-SGD parameters\n",
        "max_grad_norm = 1.0\n",
        "target_epsilon = 10.0  # Privacy budget target\n",
        "delta = 1e-5          # Delta value\n",
        "noise_multiplier = 20  # noise multiplier\n",
        "\n",
        "\n",
        "def compute_per_sample_gradients(model, loss_fn, data, targets):\n",
        "    model.zero_grad()\n",
        "    outputs = model(data)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    per_sample_grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "    return per_sample_grads\n",
        "\n",
        "def clip_gradients(gradients, max_norm):\n",
        "    total_norm = torch.norm(torch.stack([torch.norm(g.detach(), p=2) for g in gradients]))\n",
        "    clip_coef = max_norm / (total_norm + 1e-6)\n",
        "    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
        "    return [g.detach() * clip_coef_clamped for g in gradients]\n",
        "\n",
        "def add_noise(gradients, noise_multiplier, max_norm):\n",
        "    noised_gradients = []\n",
        "    for grad in gradients:\n",
        "        noise = torch.normal(0, noise_multiplier * max_norm / batch_size, grad.shape, device=grad.device)\n",
        "        noised_gradients.append(grad + noise)\n",
        "    return noised_gradients\n",
        "\n",
        "def train_step(model, optimizer, data, targets):\n",
        "    per_sample_grads = compute_per_sample_gradients(model, loss_fn, data, targets)\n",
        "    clipped_grads = clip_gradients(per_sample_grads, max_grad_norm)\n",
        "    noised_grads = add_noise(clipped_grads, noise_multiplier, max_grad_norm)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    for param, noised_grad in zip(model.parameters(), noised_grads):\n",
        "        param.grad = noised_grad\n",
        "    optimizer.step()\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    return test_loss, accuracy\n",
        "\n",
        "def compute_epsilon(steps, noise_multiplier, batch_size, dataset_size, delta):\n",
        "    \"\"\"Compute epsilon using a simplified formula.\"\"\"\n",
        "    q = batch_size / dataset_size\n",
        "    T = steps * q\n",
        "    c = math.sqrt(2 * math.log(1.25 / delta))\n",
        "    return c * math.sqrt(T) / noise_multiplier\n",
        "\n",
        "# Training loop\n",
        "steps = 0\n",
        "losses = []\n",
        "epsilons = []\n",
        "train_accuracies = []\n",
        "\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Perform a training step\n",
        "        train_step(model, optimizer, data, target)\n",
        "        steps += 1\n",
        "\n",
        "        # Compute loss for the current batch\n",
        "        outputs = model(data)\n",
        "        loss = loss_fn(outputs, target)\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Compute the number of correct predictions in the training batch\n",
        "        pred = outputs.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
        "        correct_train += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total_train += target.size(0)\n",
        "\n",
        "    # Compute average loss over the training data\n",
        "    losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = 100. * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss, test_accuracy = test(model, device, test_loader)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Train Loss: {epoch_loss / len(train_loader):.4f}, '\n",
        "          f'Train Accuracy: {train_accuracy:.2f}%, '\n",
        "          f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "    # Compute privacy spent\n",
        "    epsilon = compute_epsilon(steps, noise_multiplier, batch_size, len(train_dataset), delta)\n",
        "    epsilons.append(epsilon)\n",
        "    print(f\"Current privacy guarantee: ε = {epsilon:.2f} at δ = {delta}\")\n",
        "\n",
        "    # Stop if the current epsilon exceeds the target epsilon\n",
        "    if epsilon >= target_epsilon:\n",
        "        print(\"Target privacy budget reached. Stopping training.\")\n",
        "        break\n",
        "\n",
        "# Final privacy guarantee\n",
        "epsilon = compute_epsilon(steps, noise_multiplier, batch_size, len(train_dataset), delta)\n",
        "print(f\"Final privacy guarantee: ε = {epsilon:.2f} at δ = {delta}\")\n",
        "\n",
        "# Plotting the training loss, training accuracy, and epsilon\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "# Plot the training loss\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(losses, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Plot the training accuracy\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(train_accuracies, label='Training Accuracy', color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Plot the privacy budget (epsilon)\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(epsilons, label='Epsilon', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Epsilon (Privacy Budget)')\n",
        "plt.title('Epsilon Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}